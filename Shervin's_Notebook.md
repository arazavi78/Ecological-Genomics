# Shervin's lab notebook (Ecological Genomics BIOL 381) 

## Author: Shervin Razavi
### Affiliation:  The University of Vermont
### E-mail contact: arazavi1@uvm.edu

### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   





# Table of Contents:   
* [Entry 1: 2020-01-13, Monday](#id-section1)
* [Entry 2: 2020-01-14, Tuesday](#id-section2)
* [Entry 3: 2020-01-15, Wednesday](#id-section3)
* [Entry 4: 2020-01-16, Thursday](#id-section4)
* [Entry 5: 2020-01-17, Friday](#id-section5)
* [Entry 6: 2020-01-20, Monday](#id-section6)
* [Entry 7: 2020-01-21, Tuesday](#id-section7)
* [Entry 8: 2020-01-22, Wednesday](#id-section8)
* [Entry 9: 2020-01-23, Thursday](#id-section9)
* [Entry 10: 2020-01-24, Friday](#id-section10)
* [Entry 11: 2020-01-27, Monday](#id-section11)
* [Entry 12: 2020-01-28, Tuesday](#id-section12)
* [Entry 13: 2020-01-29, Wednesday](#id-section13)
* [Entry 14: 2020-01-30, Thursday](#id-section14)
* [Entry 15: 2020-01-31, Friday](#id-section15)
* [Entry 16: 2020-02-03, Monday](#id-section16)
* [Entry 17: 2020-02-04, Tuesday](#id-section17)
* [Entry 18: 2020-02-05, Wednesday](#id-section18)
* [Entry 19: 2020-02-06, Thursday](#id-section19)
* [Entry 20: 2020-02-07, Friday](#id-section20)
* [Entry 21: 2020-02-10, Monday](#id-section21)
* [Entry 22: 2020-02-11, Tuesday](#id-section22)
* [Entry 23: 2020-02-12, Wednesday](#id-section23)
* [Entry 24: 2020-02-13, Thursday](#id-section24)
* [Entry 25: 2020-02-14, Friday](#id-section25)
* [Entry 26: 2020-02-17, Monday](#id-section26)
* [Entry 27: 2020-02-18, Tuesday](#id-section27)
* [Entry 28: 2020-02-19, Wednesday](#id-section28)
* [Entry 29: 2020-02-20, Thursday](#id-section29)
* [Entry 30: 2020-02-21, Friday](#id-section30)
* [Entry 31: 2020-02-24, Monday](#id-section31)
* [Entry 32: 2020-02-25, Tuesday](#id-section32)
* [Entry 33: 2020-02-26, Wednesday](#id-section33)
* [Entry 34: 2020-02-27, Thursday](#id-section34)
* [Entry 35: 2020-02-28, Friday](#id-section35)
* [Entry 36: 2020-03-02, Monday](#id-section36)
* [Entry 37: 2020-03-03, Tuesday](#id-section37)
* [Entry 38: 2020-03-04, Wednesday](#id-section38)
* [Entry 39: 2020-03-05, Thursday](#id-section39)
* [Entry 40: 2020-03-06, Friday](#id-section40)
* [Entry 41: 2020-03-09, Monday](#id-section41)
* [Entry 42: 2020-03-10, Tuesday](#id-section42)
* [Entry 43: 2020-03-11, Wednesday](#id-section43)
* [Entry 44: 2020-03-12, Thursday](#id-section44)
* [Entry 45: 2020-03-13, Friday](#id-section45)
* [Entry 46: 2020-03-16, Monday](#id-section46)
* [Entry 47: 2020-03-17, Tuesday](#id-section47)
* [Entry 48: 2020-03-18, Wednesday](#id-section48)
* [Entry 49: 2020-03-19, Thursday](#id-section49)
* [Entry 50: 2020-03-20, Friday](#id-section50)
* [Entry 51: 2020-03-23, Monday](#id-section51)
* [Entry 52: 2020-03-24, Tuesday](#id-section52)
* [Entry 53: 2020-03-25, Wednesday](#id-section53)
* [Entry 54: 2020-03-26, Thursday](#id-section54)
* [Entry 55: 2020-03-27, Friday](#id-section55)
* [Entry 56: 2020-03-30, Monday](#id-section56)
* [Entry 57: 2020-03-31, Tuesday](#id-section57)
* [Entry 58: 2020-04-01, Wednesday](#id-section58)
* [Entry 59: 2020-04-02, Thursday](#id-section59)
* [Entry 60: 2020-04-03, Friday](#id-section60)
* [Entry 61: 2020-04-06, Monday](#id-section61)
* [Entry 62: 2020-04-07, Tuesday](#id-section62)
* [Entry 63: 2020-04-08, Wednesday](#id-section63)
* [Entry 64: 2020-04-09, Thursday](#id-section64)
* [Entry 65: 2020-04-10, Friday](#id-section65)
* [Entry 66: 2020-04-13, Monday](#id-section66)
* [Entry 67: 2020-04-14, Tuesday](#id-section67)
* [Entry 68: 2020-04-15, Wednesday](#id-section68)
* [Entry 69: 2020-04-16, Thursday](#id-section69)
* [Entry 70: 2020-04-17, Friday](#id-section70)
* [Entry 71: 2020-04-20, Monday](#id-section71)
* [Entry 72: 2020-04-21, Tuesday](#id-section72)
* [Entry 73: 2020-04-22, Wednesday](#id-section73)
* [Entry 74: 2020-04-23, Thursday](#id-section74)
* [Entry 75: 2020-04-24, Friday](#id-section75)
* [Entry 76: 2020-04-27, Monday](#id-section76)
* [Entry 77: 2020-04-28, Tuesday](#id-section77)
* [Entry 78: 2020-04-29, Wednesday](#id-section78)
* [Entry 79: 2020-04-30, Thursday](#id-section79)
* [Entry 80: 2020-05-01, Friday](#id-section80)
* [Entry 81: 2020-05-04, Monday](#id-section81)
* [Entry 82: 2020-05-05, Tuesday](#id-section82)
* [Entry 83: 2020-05-06, Wednesday](#id-section83)
* [Entry 84: 2020-05-07, Thursday](#id-section84)
* [Entry 85: 2020-05-08, Friday](#id-section85)


------    
<div id='id-section1'/>   


### Entry 1: 2020-01-13, Monday.   



------    
<div id='id-section2'/>   


### Entry 2: 2020-01-14, Tuesday.   



------    
<div id='id-section3'/>   


### Entry 3: 2020-01-15, Wednesday.   



------    
<div id='id-section4'/>   


### Entry 4: 2020-01-16, Thursday.   



------    
<div id='id-section5'/>   


### Entry 5: 2020-01-17, Friday.   



------    
<div id='id-section6'/>   


### Entry 6: 2020-01-20, Monday.   



------    
<div id='id-section7'/>   


### Entry 7: 2020-01-21, Tuesday.   



------    
<div id='id-section8'/>   


### Entry 8: 2020-01-22, Wednesday.   

New learned codes:

'''

git status
git add --all 
git status
git commit -m
git push 
'''


------    
<div id='id-section9'/>   


### Entry 9: 2020-01-23, Thursday.   



------    
<div id='id-section10'/>   


### Entry 10: 2020-01-24, Friday.   



------    
<div id='id-section11'/>   


### Entry 11: 2020-01-27, Monday.   



------    
<div id='id-section12'/>   


### Entry 12: 2020-01-28, Tuesday.   



------    
<div id='id-section13'/>   


### Entry 13: 2020-01-29, Wednesday.   


TO incude code as a block, use:
'''


'''



To include code in a line use ' ' example:
I am doing this blah blah blah with the command ' grep /data/' command



Red spruce project:
1-Geographically isolated populations in the NE.
2-We have to see how we can maximize the efficiency of the restoration process.
3-80,000 120bp probes designed to find
4-Paired end reads are more powerful because the beginning and the end of a sequence have match the pairs

# How to read input/output files.
-----------) you can visualize it through FastQC
------------------------------------------) you can 'trim' it with Trimmomatic
----------)trimmed.fastq file 
------------------------------------------) through the bwa program you can map (pr align) to the reference genome
-----------------------------------------) post-processing through samtools ( .bam file) also removes duplicates.

# interpretation of data
The names of the trees is in the following format:
species name_Number of mother tree- Forward or backward read(R1 oor R2)


the @ on the first line is where the gx file starts ( first line contains the machine information + barcode0

second line: base pair read
Third line: Seperator between the second line and third line
fourth iine : a code that represents the quality of the reads (it contains the Phred score)


probability of failure = 10 ^ -(phrescore)/10

This does not take into account errors in prior steps ( e.g. PCR)




###My population is  XPK          REMEMBER THIS IT IS VERY IMPORTANT

Codding:

Make sure to alwayas pull and referesh your local repository before you commit and push.

the following code is used to do so:
'''
git pull
git commit --all
git push

'''

# Other useful git commands:
'''
git status

'''


fastqc code:


![fastqc](https://github.com/arazavi78/Ecological-Genomics/blob/master/mapping_.PNG)

------    
<div id='id-section14'/>   


### Entry 14: 2020-01-30, Thursday.   



------    
<div id='id-section15'/>   


### Entry 15: 2020-01-31, Friday.   



------    
<div id='id-section16'/>   


### Entry 16: 2020-02-03, Monday.   



------    
<div id='id-section17'/>   


### Entry 17: 2020-02-04, Tuesday.   



------    
<div id='id-section18'/>   


### Entry 18: 2020-02-05, Wednesday.   

Today we will be focusing on mapping to a subset of the reference genome.

Notes:

For the mapping scripts, we have to give both the forward and reverse reads of each individual and map it to the reference genome.  This will generate a sam file which can further on be converted to a bam file that is readable.


The Process_bam.sh script converts the sam files to bam files, deletes the PCR duplicates and  converts the final output to index files.
 
 
 we use seperate for loops for the processes used in this script.
 
 
 It is impotant to note that the PCR duplicates are removed because they lead to a lot of false positive calls.


Make sure to copy file locations accurately.


The mypipeline.sh script includes the two other scripts so that the "mother" defined variables (found in the mypipeline.sh script) do not have to be signified over and over again.

To test if it is working, make sure you have the main variables loaded on the system so you can run the other two scripts from outside of the mypipeline.sh script.

Pciture of the code:


![code](https://github.com/arazavi78/Ecological-Genomics/blob/master/mapping.PNG)

![code](https://github.com/arazavi78/Ecological-Genomics/blob/master/mapping_.PNG)




------    
<div id='id-section19'/>   


### Entry 19: 2020-02-06, Thursday.   



------    
<div id='id-section20'/>   


### Entry 20: 2020-02-07, Friday.   



------    
<div id='id-section21'/>   


### Entry 21: 2020-02-10, Monday.   



------    
<div id='id-section22'/>   


### Entry 22: 2020-02-11, Tuesday.   



------    
<div id='id-section23'/>   


### Entry 23: 2020-02-12, Wednesday.   

# Sam file stuff:


the second number is the flag number which gives you information about the read ( you can look up the meaning of the "flag" number  in the internet).
The third number is the place where the contig mapped to.
The forth number is the left position of the read
The fifth number is the quality number ( the higher the better)


samtools flagstat was preformed on the *.sam files.


Depth of coverage :How many reads stack up against any single position in the reference genome.


Samtools t-view ---) is a lightweight viewer


Genotype likelihood takes into account all of the possibilities that exist and does not simply go with the most probable situation.


ANGSD is a pretty cool platform you will use often!



# Workflow of ANGSD---) 
first site frequency spectrum (SNP frequency).
2- estimate nucleotide diversities
3-estiemate fst between all populations or pairwise between sets of populations.
perfom a genetic PCA based on estimatio of the genetic covariance matrix ( this 


The final code is very interesting and has a lot of relevant genomic information 

look up the meaning of the ANGSD command options.

# Major allele : most frequent allele.


# New commands:
use the "tail" command to get to the end of the file.

Today's code:

![ANGSD](https://github.com/arazavi78/Ecological-Genomics/blob/master/ANGSD.PNG)


------    
<div id='id-section24'/>   


### Entry 24: 2020-02-13, Thursday.   



------    
<div id='id-section25'/>   


### Entry 25: 2020-02-14, Friday.   



------    
<div id='id-section26'/>   


### Entry 26: 2020-02-17, Monday.   



------    
<div id='id-section27'/>   


### Entry 27: 2020-02-18, Tuesday.   



------    
<div id='id-section28'/>   


### Entry 28: 2020-02-19, Wednesday.   

Folded SFS is used for cases that we do not know their clear ancestry.




the bottleneck is inherited from the ancestor.

Because Tajima's D is bigger than 1, the populations are porbably undergoing the same selective pressure.


Small values of theta and pi, hence not a lot going on.

The species not growing ( it is tanking because of the positive value of Tajima's D)

next step:


ANGSD_mypop.sh:


Make sure for the final analysis to not have the 


fold 1 is used to indicate the usage of the folded sfs.

coding note: make sure not to use "\" on the last option of the ANGSD command


   which particular SNPs are contributing to the local adaptations??------------------) GWAs







------    
<div id='id-section29'/>   


### Entry 29: 2020-02-20, Thursday.   



------    
<div id='id-section30'/>   


### Entry 30: 2020-02-21, Friday.   



------    
<div id='id-section31'/>   


### Entry 31: 2020-02-24, Monday.   


Transcriptomics : -Experimentally hold/control environmental conditions constant to reveal genetically controlled differences of phenotypes at the molecular level.

Phenotypes: Characteristics of an organism that can be observed. Disease resistance/susceptibility (cellular level) or color (morphological scale).

Central dogma of molecular bioloy: DNA ------(transcription-----) RNA -----(Translation)------) Protein -------) Phenotype


Reverse Ecology ----) Like GWAS



mutations ( at the DNA level):-----) -coding regions (Change of amino acids) 
                  -splice site variation
                  -promoter enhancer- expression level.
                           -expression level
                           -timing of expression 
                           -cell/tissue typpe of expressed gene 
                           -conditions under which a gene is expressed  (e.g. ogranisms that have diverged in their diversity of heat                                   stress
Epigenetic modificaitons

                           
Post translational modifications ( at the Protein level)




P = G + E + (G*E)
Experimentally induce yout phenotype of interest and control the environment --------------) molecular underpinnings of phenotypes.


Factors :
         - Controlling the envionmental condiiton (common garden)
         - Treatments/Conditions that you  are interested in
         - Populations of interest
         - Tissue that is being sampled
         - Life histoory change
         - Transgenerational enviornment 
         - Sexual dimorphism ( Sex and Reproductive stage )
         
         
HOW can we compare different species 

Work flow :
           1-Careful experimental design
               -Questions Hypotheses ( it could be broad [*omics] or a priori hypotheses]
           2- Experiment
                - sample tissues/individuals
                - sample RNA - labile           mRNA
                -Extract, prep, sequence        3' tag 
           3- Process + analyze data
               Quality check ----) Clean ----) qualiy check (.fastq)
               map to ref + Extract count data
               Normalization of data  -----------)) generation of data mattrix.
           4- integrate: functional enrichment aalyses, network analyses
                        and you can integrate with other data types such as SNP and TajD, pi, microbiome, epigenetic data, proteomics,                            environment.       
------    
<div id='id-section32'/>   


### Entry 32: 2020-02-25, Tuesday.   



------    
<div id='id-section33'/>   


### Entry 33: 2020-02-26, Wednesday.   

#### Learning objectives for today:

1-Review Red Spruce ecology and biogeography and the transcriptomics experimental design.
2-Articulate the questions we can address and hypotheses we can test with this experimental design.
3-Understand the general work flow or “pipeline” for processing and analyzing RNAseq data.
4-Review how to make/write a bash script and how write a script to process files in batches.
5-Visualize and interpret Illumina data quality (Run FastQC on raw and cleaned reads).
6-Start mapping reads and quantifying abundance simultaneously using Salmon.
7-Import quant.sf files generated by Salmon into DESeq2 for analysis and visualization.
8-Add to your growing list of bioinformatics tricks (take notes!).


Today we will work on transcriptomics:

#### Experimental Design:
##### 1-Ten maternal families total:

sample labels are “POP_FAM” ASC_06, BRU_05, ESC_01, XBM_07, NOR_02, CAM_02, JAY_02, KAN_04, LOL_02, MMF_13

##### 2-Two Source Climates:

(SourceClim: HotDry (5 fams): ASC_06, BRU_05, ESC_01, XBM_07, NOR_02 and CoolWet (5 fams): CAM_02, JAY_02, KAN_04, LOL_02, MMF_13)

##### 3-Experimental Treatments (Trt):

Control: watered every day, 16:8 L:D photoperiod at 23C:17C temps
Heat: 16:8 L:D photoperiod at 35C:26C temps (50% increase in day and night temps over controls)
Heat+Drought: Heat plus complete water witholding

##### 4-Three time periods (Day):

Harvested tissues on Days 0, 5, and 10
Extracted RNA from whole seedlings (root, stem, needle tissue)
Aimed for 5 biological reps per Trt x SourceClim x Day combo, but day5 had few RNA extractions that worked




#### Questions that we could answer given the experimental desiggn ( two origin pops, 3 treatments, 3 experimental days).

1- Do individuals from different sources have different responses to different enviornments or different time points?
  you  can make regression models with interaction variables. Basically, you can make the interactions between the three factors.

2- Do families from different source climates differ in gene expression?


3- Is there a transcriptome wide response to heat stress? Does this change when the additional stress of drought is added


4- Which specific genes are involved in the above response?5-Do DE genes or pathways show evidence of positive selection?

6- Association mapping.



##### My populations are  #NOR H and XBM C


#### Data Processing Pipeline:
1-FastQC on raw reads –> Trimmomatic (done!) –> FastQC on cleaned reads

2-Reference transcriptome: 

` /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies1.0-all-cds.fna.gz Downloaded from Congenie.org `

3-66,632 unigenes, consisting of 26,437 high-confidence gene models, 32,150 medium-confidence gene models, and 8,045 low-confidence gene models

4-Use Salmon to simulateously map reads and quantify abundance.

5-Import the data into DESeq2 in R for data normalization, visualization, and statistical tests for differential gene expression.



### fastqc and trimmomatic

Professor Pespeni has done all reads, but as a refresher:

#### trim_loop.sh (found in myscripts on github)
```
#!/bin/bash

cd /data/project_data/RS_RNASeq/fastq/

########## Trimmomatic for single end reads

for R1 in *R1.fastq.gz  



do 
    echo "starting sample ${R1}"
    f=${R1/_R1.fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticSE \
        -threads 1 \
        -phred33 \
         "$R1" \
         /data/project_data/RS_RNASeq/fastq/cleanreads/${name}_R1.cl.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-SE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        HEADCROP:12 \
        MINLEN:35 
     echo "sample ${R1} done"
done

```


#### fastqc_trimmed.sh

```

#!/bin/bash

mkdir fastqc_trimmed

cd ~/Ecological-Genomics/myresults/fastqc_trimmed

for file in /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/XPK*.cl.pd.fq

do

fastqc ${file} -o ./

done


```


#### Mapping (Salmon)

Use Salmon to quantify transcript abundance

###### First step: Index the reference transcriptome. This only needs to be done once and has been done already, but here’s the code:
```
cd /data/project_data/RS_RNAseq/ReferenceTrancriptome/
salmon index -t Pabies1.0-all-cds.fna.gz -i Pabies_cds_index --decoys decoys.txt -k 31
```


###### Second step: Start quantification! Let’s see if we can write a for loop together to start the mapping based on the Salmon tutorial

#### Salmon.sh file for my populations:
```

for file in NOR_02_H*R1.cl.fq
do

salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_                                         index -l A -r ${file} --validateMappings -o /data/project_data/RS_RNASeq/salmon/                                         cleanedreads/${file}


done





for file in XBM_07_C*R1.cl.fq

do

salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_                                         index -l A -r ${file} --validateMappings -o /data/project_data/RS_RNASeq/salmon/                                         cleanedreads/${file}


done


```

#### the relevant libraries + the programs DESeq2 were installed.






##### Notes on the pipeline:


1-The headcrop comman was used in the trimmomatic program to delete the first 12 base pairs.


You can se the effect comparing the fastqc of the cleaned reads and the normals reads.


2-For the overrepresented sequences, the fastqc gives you data on the percentage the reads have been found and also the length of it.



------    
<div id='id-section34'/>   


### Entry 34: 2020-02-27, Thursday.   



------    
<div id='id-section35'/>   


### Entry 35: 2020-02-28, Friday.   



------    
<div id='id-section36'/>   


### Entry 36: 2020-03-02, Monday.   

# W hat is differential expression?
## Differences in transcript abundance b/w experimental groups --------) Genes that are changing

Sample prep ---------------) RNA extraction -----------------) Library prep --------------) SequNencing -----------)
Quality control (fastQC and trimming) -----------) mapping and counting (Salmon)

# Normalization: 
data matrix/gene count table
It is important to normalize fr library size.
Library composition
Question: 
How do you acccount for/analyze the  global differences betweeen samples

Normalization
Counts per million(CPM)) - Depth
Transcripts per Kilobase million-----------) Depth and gene lenth
Fragments per Kilobase million ------------) Depth and gene length
Edge R -----) Depth, Gene size and library composition 
DESeq2 -----) Depth and composition 

Differential Expression:

You have to take into account false positives and false negatives

Independent filtering :
        Limiting the number of tests based on a cut-off (basemean)
        Benamin, Hochberg(BH)
              Adjusted p-value
                    Control False Positives
                    FDR-False 
                    
Generalized Linear Model (GLM)
-------) Negative binomial distribution
Waod test -----) p-value



# Visualization:
   Sequencing summary statistics
   barplos of total read counts ( You can do # null genes)
   
   After normalization you can do a normalized count analysis.
   
   You can also do PCA
   
   Gene heat mapping (Uses hierarchical clustering to have the genes/ samples that are similar closer together).
   
   Ven diagrams: You can split up up-regulated genes, down-regulated genes, etc...
   
   MA plots ----) Log of FC [full change] over log of mean expression.
   
   
   Volcano plots : on one access you have log [full change] vs -log [p-value]
   
   log FC scatter plot: Log FC of condition 1 vs Log FC condition 2 
   
   individual gene treatment response curves :
   
   Examples: Relative Full Change vs treatment
   
## Question
You keep talking about ratio differences and their importance. How about the absolute transcriptomic differences. What if we have the same ratios, but 1  tenth of the transcription????





# Enrichment Analysis:
Looking at gene sets from functional databases ( Gene Ontology [GO])
      ----) Molecular function      -----) Example Adenylne cyclase activity
      ----) Biological process      -----) Puridimine biosynthesis
      ----) Cell Compartment        -----) Ribozome
      
      Another databse: KEGG pathway:
      Genetic Information Processing 
      ------) Transcription
      ------) Splicosome 
      
   
      Met
      hods: 
             Overrepresentation analysis 
             DEG ----) 2200 (ALL)        24(gene set)             18  (gene set 20
             total -----) 15,000           75                     120
             ------------) 14% of the genome is differentially expresse/d.                 32%         15%      
             
             
------    
<div id='id-section37'/>   


### Entry 37: 2020-03-03, Tuesday.   



------    
<div id='id-section38'/>   


### Entry 38: 2020-03-04, Wednesday.   


#### Today's code (salmon_trimmed.sh):

```
for file in /data/project_data/RS_RNASeq/fastq/cleanreads/NOR*C*R1.cl.fq

do 

salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_H27_index -l A -r file --validateMappings -o /data/project_data/RS_RNASeq/salmon/cleanedreads

done 





for file in /data/project_data/RS_RNASeq/fastq/cleanreads/XBM_07_H*R1.cl.fq

do

salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_H27_index -l A -r file --validateMappings -o /data/project_data/RS_RNASeq/salmon/cleanedreads

done
```

### important coding notes to consider for the future:

1-Before starting, make sure your destinations are accurate.


2- Check for typos from the very beginnning 



 #### finding the mapping rate for all samples:
For each sample mapped, you now have a directory with several output files including a log of the run. In that log, the mapping rate (% of reads mapped with sufficient quality) is reported. We can view the contents of the file using cat. We can also use grep (i.e., regular expressions) to pull out the mapping rate for all the samples. Though there’s probably a more elegant solution, here is one:

` grep -r --include \*.log -e 'Mapping rate' `


 
#### notes
1-Mapping rate = Percent of the reads in each of the files that mapped to the genome ( not the other way around!)

2-High confidence----) 70 % or higher

3-Median 30-70 % percent, low = >0%

4-Pabies_cds_index was used as the indexx ( it contains all reads). is improved the percent mapping significantly.

### combining the quant.sf files:

We can use the the `R` package `tximport`. 

### `R` code:


``` library(tximportData)
library(tximport)

#locate the directory containing the files. 
dir <- "/data/project_data/RS_RNASeq/salmon/"
list.files(dir)

# read in table with sample ids
samples <- read.table("/data/project_data/RS_RNASeq/salmon/RS_samples.txt", header=TRUE)

# now point to quant files
all_files <- file.path(dir, samples$sample, "quant.sf")
names(all_files) <- samples$sample

# what would be used if linked transcripts to genes
#txi <- tximport(files, type = "salmon", tx2gene = tx2gene)
# to be able to run without tx2gene
txi <- tximport(all_files, type = "salmon", txOut=TRUE)  
names(txi)

head(txi$counts)

countsMatrix <- txi$counts
dim(countsMatrix)
#[1] 66069    76

# To write out
write.table(countsMatrix, file = "RS_countsMatrix.txt", col.names = T, row.names = T, quote = F) 

```



####  results and discussion :

One possibility is that there is not a lot of conifer specific genes on genebank. ( maybe that is why the mapping rate is so low)


The reads had the highest mapping rate with the wholle index file (cds) rather than the HC and the HC27 index files.


The salmon index command works with the gz files ( you can change the kmer flag ( -k ), if you make it smaller it might make it better but it didn't make it better in our investigations. 


The mapping rates between the --seqbias and the normal ones were identical for the cleaned reads because they are clean!!!!!!


The quant.sf file is where the main info is.


![Example of quant.sf data ](https://github.com/arazavi78/Ecological-Genomics/blob/master/quant.sf.PNG)

  
 
------    
<div id='id-section39'/>   


### Entry 39: 2020-03-05, Thursday.   



------    
<div id='id-section40'/>   


### Entry 40: 2020-03-06, Friday.   



------    
<div id='id-section41'/>   


### Entry 41: 2020-03-09, Monday.   



------    
<div id='id-section42'/>   


### Entry 42: 2020-03-10, Tuesday.   



------    
<div id='id-section43'/>   


### Entry 43: 2020-03-11, Wednesday.   



------    
<div id='id-section44'/>   


### Entry 44: 2020-03-12, Thursday.   



------    
<div id='id-section45'/>   


### Entry 45: 2020-03-13, Friday.   



------    
<div id='id-section46'/>   


### Entry 46: 2020-03-16, Monday.   



------    
<div id='id-section47'/>   


### Entry 47: 2020-03-17, Tuesday.   



------    
<div id='id-section48'/>   


### Entry 48: 2020-03-18, Wednesday.   


#### How we improved mapping rates:

Recall that two weeks ago you all wrote for loops to map your set of cleaned fastq files to the reference transcriptome. We discovered that we had low mapping rates, ~2%! We did some troubleshooting in class. We further hypothesized that many of our reads were not mapping because the reference we had selected included only the coding region. In working with 3’ RNAseq data, much of our sequencing effort is likely to be in the 3’ UTR (untranslated region). We concatenated the reference sequences for the coding (“cds”) and the 3’ UTR (“2kb downstream”) for each gene. We then mapped to this new reference using salmon (as you had done before). Our mapping rate improved dramatically, ranging from 40-70% of reads mapping across samples, mean of 52%.

#### How to import counts matrix and sample ID tables into R and DESeq2

We have used the following code ( this is the completed version) for these purposes:


#  DESeq2_RS_classLive_day10start.R file (in the myscripts directory)
##  DESeq2_RS_classLive contains the same codes but for all samples not just day 10
## R code for DESeq2 experimental designs and GLMs

```

## Set your working directory
setwd("C:/Users/reals/Desktop/Rstudio")

## Import the libraries that we're likely to need in this session
library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")


## Import the counts matrix
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable) # Need to round because DESeq wants only integers
head(countsTableRound)

## Import the samples description table - links each sample to factors of the experimental design.
# Need the colClasses otherwise imports "day" as numeric which DESeq doesn't like, coula altneratively change to d0, d5, d10
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)

############ Try with only Day 10 data

# grep("10", names(countsTableRound), value = TRUE)
# day10countstable <- subset(countsTableRound, grep("10", names(countsTableRound), value = TRUE)) #doesn't work has to be logical

day10countstable <- countsTableRound %>% select(contains("10"))
dim(day10countstable)

conds10<- subset(conds, day=="10")
dim(conds10)
head(conds10)

## Let's see how many reads we have from each sample:
colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

# What's the average number of counts per gene
rowSums(countsTableRound)
mean(rowSums(countsTableRound))
median(rowSums(countsTableRound))
# wow! This shows dispersion across genes - differences in magnitude of expression

# What's the average number of counts per gene per sample
apply(countsTableRound,2,mean)

## Create a DESeq object and define the experimental design here with the tilde

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData = conds, 
                              design = ~ climate + day + treatment)
dim(dds)
# [1] 66408    76

# Filter out genes with few reads 

dds <- dds[rowSums(counts(dds)) > 76]
dim(dds)
# [1] 23887    76  This is filtering to sum of 76 reads across all samples
# [1] 7884   76  This is filtering to sum of 760 reads across all samples

## Run the DESeq model to test for differential gene expression: 
# 1) estimate size factors (per sample), 2) estimate dispersion (per gene), 
# 3) run negative binomial glm
dds <- DESeq(dds)

# List the results you've generated
resultsNames(dds)
# Running the model: design = ~ pop + day + treatment
# [1] "Intercept"            "pop_BRU_05_vs_ASC_06" "pop_CAM_02_vs_ASC_06"
# [4] "pop_ESC_01_vs_ASC_06" "pop_JAY_02_vs_ASC_06" "pop_KAN_04_vs_ASC_06"
# [7] "pop_LOL_02_vs_ASC_06" "pop_MMF_13_vs_ASC_06" "pop_NOR_02_vs_ASC_06"
# [10] "pop_XBM_07_vs_ASC_06" "day_10_vs_0"          "day_5_vs_0"          
# [13] "treatment_D_vs_C"     "treatment_H_vs_C"

# Running the model: design = ~ climate + day + treatment
# [1] "Intercept"        "climate_HD_vs_CW" "day_10_vs_0"      "day_5_vs_0"      
# [5] "treatment_D_vs_C" "treatment_H_vs_C"


# Order and list and summarize results from specific contrasts
# Here you set your adjusted p-value cutoff, can make summary tables of the number of genes differentially expressed (up- or down-regulated) for each contrast
res <- results(dds, alpha = 0.05)
res <- res[order(res$padj),]
head(res)

# log2 fold change (MLE): treatment H vs C 
# Wald test p-value: treatment H vs C 
# DataFrame with 6 rows and 6 columns
# baseMean    log2FoldChange             lfcSE              stat
# <numeric>         <numeric>         <numeric>         <numeric>
#   MA_172878g0010   15.8548874481417  2.26899213338594  0.44065452286252  5.14914068882447
# MA_107783g0020    6.6082118492291 -1.96824414729957 0.394042285152064 -4.99500744327481
# MA_28973g0010    18.8813749792546  -1.9664671947209 0.412333404130031 -4.76911929769524
# MA_10434037g0010   5.611769238156   2.1853605976071  0.49671691935347  4.39960974240937
# MA_10426002g0010 10.8980752578363 -1.20767724886483 0.283132420279441 -4.26541491671245
# MA_10429525g0010 60.5937645508928  1.17170086164903 0.281505776107537  4.16226223792093
# pvalue                padj
# <numeric>           <numeric>
#   MA_172878g0010   2.61682549726074e-07 0.00249278796869058
# MA_107783g0020   5.88334982423675e-07 0.00280223952128396
# MA_28973g0010    1.85033065465192e-06 0.00587541660540472
# MA_10434037g0010  1.0844572516541e-05  0.0258263494481425
# MA_10426002g0010 1.99531043147766e-05  0.0357522692442608
# MA_10429525g0010 3.15110173906545e-05  0.0357522692442608

summary(res)
# out of 23887 with nonzero total read count
# adjusted p-value < 0.05
# LFC > 0 (up)       : 16, 0.067%
# LFC < 0 (down)     : 3, 0.013%
# outliers [1]       : 61, 0.26%
# low counts [2]     : 14300, 60%

res_treatCD <- results(dds, name="treatment_D_vs_C", alpha=0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
head(res_treatCD)
# log2 fold change (MLE): treatment D vs C 
# Wald test p-value: treatment D vs C 
# DataFrame with 6 rows and 6 columns
# baseMean   log2FoldChange             lfcSE             stat
# <numeric>        <numeric>         <numeric>        <numeric>
#   MA_10257300g0010 20.9979917001674  6.3160877571623 0.761778438070059 8.29123986911983
# MA_444738g0020   23.5872071084088 2.60491779951534 0.331247263623815 7.86396775332654
# MA_57964g0010    7.89927388331396 5.39652442842906 0.688793826922627 7.83474563432065
# MA_75192g0010    37.9183573851468 5.81210235837303 0.768762098604424 7.56033936756775
# MA_10428616g0010  35.758883777048 3.82582283371241 0.510641392538861 7.49219097709799
# MA_7017g0010     64.7924705055064 2.64439151272771 0.360360746749988 7.33817857959526
# pvalue                 padj
# <numeric>            <numeric>
#   MA_10257300g0010 1.12074011571854e-16 1.84462615646114e-12
# MA_444738g0020   3.72153387856494e-15 2.57743989393094e-11
# MA_57964g0010    4.69792799185419e-15 2.57743989393094e-11
# MA_75192g0010    4.02019076294682e-14 1.65420799418354e-10
# MA_10428616g0010 6.77332669682435e-14 2.22964368206064e-10
# MA_7017g0010     2.16520151829775e-13 5.93950863161046e-10

summary(res_treatCD)
# out of 23887 with nonzero total read count
# adjusted p-value < 0.05
# LFC > 0 (up)       : 678, 2.8%
# LFC < 0 (down)     : 424, 1.8%
# outliers [1]       : 61, 0.26%
# low counts [2]     : 7367, 31%

```


#Data visualization


## MA plot

```


plotMA(res_treatCD,ylim=c(-3,3))
```

## PCA

```
vsd <- vst(dds, blind=FALSE)

data <- plotPCA(vsd,intgroup=c("climate","treatment","day"),returnData=TRUE)
percentVar <- round(100 * attr(data, "percentVar"))

data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("C","H","D"))
data$day <- factor(data$day, levels=c("0","5","10"), labels = c("0","5","10"))


ggplot(data, aes(PC1, PC2, color=pop, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

# Counts of specific top gene! (important validatition that the normalization, model is working)
d <-plotCounts(dds, gene="MA_7017g0010", intgroup = (c("treatment","climate","day")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=day, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p

p <-ggplot(d, aes(x=treatment, y=count, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p

```



## Heatmap of top 20 genes sorted by pvalue


```
library(pheatmap)
topgenes <- head(rownames(res_treatCD),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate","day")])
pheatmap(mat, annotation_col=df)

```




## Additional notes from today's class

Learning how to use the online format.

Change day with climate and run the program again.

congeini is a tool that gives you information on genes and their structures.




------    
<div id='id-section49'/>   


### Entry 49: 2020-03-19, Thursday.   



------    
<div id='id-section50'/>   


### Entry 50: 2020-03-20, Friday.   



------    
<div id='id-section51'/>   


### Entry 51: 2020-03-23, Monday.   



------    
<div id='id-section52'/>   


### Entry 52: 2020-03-24, Tuesday.   



------    
<div id='id-section53'/>   


### Entry 53: 2020-03-25, Wednesday.   

# Epigenetics

#### Experimental species: Acartia tonsa

Acartia tonsa is a calanoid copepod that has a world wide distribution. It inhabits estuaries and coastal waters and it typically the most abundant zooplankton present. Because of their huge population sizes and global distribution, they play an important role in ocean biogeochemical cycling and ecosystem functioning. For example, they’re an important food base for many fishes. Given their broad distribution in dynamic habitats (estuaries), they can tolerate and live in a broad range of salinities, freshwater to seawater, and temperatures, among other stressors.

We know that the world is rapidly changing due to human activities and it is important to understand how species will respond to these changes. We were interested in understanding how A. tonsa could respond to stressful environments that will likely be caused by climate change. Can they adapt to rapid changes in temperature and acidity? How might epigenetic responses interact with adaptation to enable resilience?

A. tonsa is a great system to ask these questions for a number of reasons. First, their generation time is only ~10-20 days and they’re tiny! Only 1-2 mm. This means we can easily execute multi-generational studies with thousands of individuals. Second, because they can tolerate such variable environments, they should have lots of plasticity to respond to environmental fluctuations. Finally, their large population sizes mean that genetic diversity should be high, giving them high adaptive potential. This means that if any species can respond to rapid environmental change, it is likely A. tonsa.

#### Experimental design

##### 4 treatments:

1-Control

2-Hgh temperature

3-High CO2

4- High temperature + High CO2

We collected A. tonsa from the wild, common gardened them for three generations, then split them into four treatments with four replicates each and about 3,000-5,000 individuals per replicate. We left them to evolve in these conditions for 25 generations.

Samples were collected at generation F0 and F25 to quantify methylation frequencies using reduced representation bisulfite sequencing (RRBS). 

#### Reduced representation bisulfite sequencing (RRBS)

Following the adapter ligation, we bisulfite convert all unmethylated C’s to T’s.

Before starting we also spiked in a small amount of DNA from E. coli that we know wasn’t methylated. Using this, we can calculate downstream how efficient our bisulfite conversion was.


#### Pipeline
1-Visualize, clean, visualize
    You won’t have to do this step
    Visualize quality of raw data with fastqc
    clean raw data with Trimmomatic
    Visualize quality of cleaned data with fastqc

2-Align to Acartia tonsa reference genome (Bismark)
    also align lambda DNA to check for conversion efficiency (Done for you)

3-Extract methylation calls

4-Process and filter calls

5-Summary figures (PCA, clustering, etc)

6-Test for differential methylation (Methylkit)


#### Why do bisulfate sequence data look different?

---------------------)  bisulfite conversion! And don’t forget that the reverse strand is the reverse complement of the bisulfite converted forward strand, and is not just the BS converted bottom strand. See image below:



#### bismark.sh

Questions : 
Why is this different from typical DNA alignment?

What do we need to do to the genome?

##### Code:

```
#!/bin/bash

bismark --bowtie2 --multicore 1 \
    --genome /data/project_data/epigenetics/reference_genome \
    --output_dir /data/project_data/epigenetics/aligned_output \
    -1 /data/project_data/epigenetics/trimmed_fastq/SAMPLEID_1.fq.gz \
    -2 /data/project_data/epigenetics/trimmed_fastq/SAMPLEID_2.fq.gz \
    --rg_tag --rg_id SAMPLEID --rg_sample SAMPLEID --gzip --local --maxins 1000
    
    
```



#### bismark parameters:

--bowtie2 tells bismark to map with bowtie2. There are other options possible here (bowtie1, for example)
--multicore 1 Use one core. We could use multiple cores to make alignment faster, but we may crash the server if many of you are mapping at once if we do this.
--genome the location of our converted genome. The bismark package has a command to prep your genome that I've already done. You can check it out: bismark_methylation_extractor --help
-1 the path to your sample's forward read
-2 the path to your sample's reverse read
--rg_tag add a read group tag that identifies your individual sample in your output bam file
--rg_id the string that defines the readgroup ID
--rg_sample the string that defines your readgroup sample ID
--gzip for the temporary files, use gzip to save space
--local align with the local alignment option in bowtie2. This will include soft clipping, which should increase mapping rate, but comes at the cost of (maybe) increasing mis-mapping. 
--maxins specifies the maximum insert size for mapping. We're using 1000 as our libraries had a mean insert size of ~200. Note that this is smaller than most sequencing libraries.

#### Bismark differences with bowtie2:

Bismark is, at its core, running bowtie2. But there are important differences. If you remember, we’re using two modified versions of the genome where we’ve converted C-to-T AND G-to-A. We also generate temporary files of our trimmed reads where we convert C-to-T (read1) AND G-to-A (read2) so they can map to this converted genome. This makes the alignment a bit harder because 1. the complexity of DNA is reduced, and 2. we are mapping to two separate genomes.


------    
<div id='id-section54'/>   


### Entry 54: 2020-03-26, Thursday.   



------    
<div id='id-section55'/>   


### Entry 55: 2020-03-27, Friday.   



------    
<div id='id-section56'/>   


### Entry 56: 2020-03-30, Monday.   



------    
<div id='id-section57'/>   


### Entry 57: 2020-03-31, Tuesday.   



------    
<div id='id-section58'/>   


### Entry 58: 2020-04-01, Wednesday.   

# Epigenetics day 2


#### Code to extract methylation data:

```
bismark_methylation_extractor --bedGraph --scaffolds --gzip \
    --cytosine_report --comprehensive \
    --no_header \
    --parallel 6 \
    --output ~/tonsa_epigenetics/analysis/methylation_extract/ \
    --genome_folder /data/copepods/tonsa_genome/ \
    *pe.bam
```

#### To ignore first two bases :

```

bismark_methylation_extractor --bedGraph --scaffolds --gzip \
    --cytosine_report --comprehensive \
    --no_header \
    --parallel 6 \
    --ignore 2 --ignore_r2 2 \
    --output ~/tonsa_epigenetics/analysis/methylation_extract/ \
    --genome_folder /data/copepods/tonsa_genome/ \
    *pe.bam
```
#### Link to methylation extraction report ignoring bias

```

Column 1: Chromosome
Column 2: Start position
Column 3: End position
Column 4: Methylation percentage
Column 5: Number of methylated C's
Column 6: Number of unmethylated C's

```



#### We can look at sites with some data using the following:

``` 
zcat HH_F25_4_1_bismark_bt2_pe.bismark.cov.gz | awk '$4 > 5 && $5 > 10' | head

```

### Methylkit analysis code:
```
library(methylKit)
library(tidyverse)
library(ggplot2)
library(pheatmap)

# first, we want to read in the raw methylation calls with methylkit

# set directory with absolute path (why is this necessary? I have no idea, but gz files wont work with relative paths)
dir <- "/Users/reidbrennan/Documents/UVM/Ecological_genomics_teaching/data"

# read in the sample ids
samples <- read.table("~/Documents/UVM/Ecological_genomics_teaching/data/sample_id.txt", header=FALSE)

# now point to coverage files
files <- file.path(dir, samples$V1)
all(file.exists(files)) # checking that all files exist

# convert to list
file.list <- as.list(files)

# get the names only for naming our samples
nmlist <- as.list(gsub("_1_bismark_bt2_pe.bismark.cov.gz","",samples$V1))

# use methRead to read in the coverage files
myobj <- methRead(location= file.list,
        sample.id =   nmlist,
                      assembly = "atonsa", # this is just a string. no actual database
                      dbtype = "tabix",
                      context = "CpG",
                      resolution = "base",
                      mincov = 20,
                            treatment = 
                              c(0,0,0,0,
                                1,1,1,1,
                                2,2,2,2,
                                3,3,3,3,
                                4,4,4,4),
                      pipeline = "bismarkCoverage",
                      dbdir = "~/Documents/UVM/Ecological_genomics_teaching/data/")

######
# visualize coverage and filter
######

# We can look at the coverage for individual samples with getCoverageStats()
getCoverageStats(myobj[[1]], plot = TRUE, both.strands = FALSE) #Get CpG coverage information

# but can plot all of our samples at once to compare.
par(mfrow=c(5,4))

for(i in 1:length(myobj)) {
  getCoverageStats(myobj[[i]], plot = TRUE, both.strands = FALSE) #Get CpG coverage information
} #Plot and save %CpG methylation information


# filter samples by depth with filterByCoverage()
filtered.myobj=filterByCoverage(myobj,lo.count=20,lo.perc=NULL,
                                      hi.count=NULL,hi.perc=97.5,
                                      db.dir = "~/Documents/UVM/Ecological_genomics_teaching/data/")

######
# merge samples
######

#Note! This takes a while and we're skipping it

# use unite() to merge all the samples. We will require sites to be present in each sample or else will drop it

meth <- unite(filtered.myobj,mc.cores=3, suffix="united",
              db.dir = "~/Documents/UVM/Ecological_genomics_teaching/data/")
         
```

#### loading from databases


Methylkit has a convenient aspect where we can load previously generated databases. This means we don’t have to re-run analyses, but can skip ahead to where we previously left off. We will do this below to load the united database that I’ve already generated.
```
meth <- methylKit:::readMethylBaseDB(
                      dbpath = "~/Documents/UVM/Ecological_genomics_teaching/data/methylBase_united.txt.bgz",
                            dbtype = "tabix",
                            sample.id =   unlist(nmlist),
                            assembly = "atonsa", # this is just a string. no actual database
                            context = "CpG",
                            resolution = "base",
                            treatment = c(0,0,0,0,
                              1,1,1,1,
                              2,2,2,2,
                              3,3,3,3,
                              4,4,4,4),
                            destrand = FALSE)
```

#### Methylation statistics across samples
```
# percMethylation() calculates the percent methylation for each site and sample

pm <- percMethylation(meth) # get percent methylation matrix

ggplot(gather(as.data.frame(pm)), aes(value)) + 
    geom_histogram(bins = 10, color="black", fill="grey") + 
    facet_wrap(~key) # can add  scales = 'free_x'

sp.means <- colMeans(pm)
p.df <- data.frame(sample=names(sp.means),
          group = substr(names(sp.means), 1,6),
          methylation = sp.means)

ggplot(p.df, aes(x=group, y=methylation, color=group)) + 
    stat_summary(color="black") + geom_jitter(width=0.1, size=3)
```    
### Summarize variation: PCA, clustering

```
clusterSamples(meth, dist="correlation", method="ward.D", plot=TRUE)

PCASamples(meth, screeplot=TRUE)
PCASamples(meth, screeplot=FALSE)
```


### find differentially methylated sites between two groups

```

# subset with reorganize()

meth_sub <- reorganize(meth,  sample.ids= (c("AA_F00_1","AA_F00_2","AA_F00_3", "AA_F00_4",
                                          "HH_F25_1","HH_F25_2","HH_F25_3","HH_F25_4")), 
                              treatment=c(0,0,0,0,1,1,1,1),
                             save.db=FALSE)
                             
# calculate differential methylation

myDiff=calculateDiffMeth(meth_sub,
            overdispersion="MN",
            mc.cores=8,
            suffix = "full_model", adjust="qvalue",test="Chisq")

# where MN corrects for overdispersion
# fit a logistic regression to methylation values where explanatory variable is the treatment (case or control). 
# and we compare the fit of the model with explanatory variable vs the null model (no explanatory variable) 
and ask if the fit is better using a Chisq test. 
# the methylation proportions are weighted by their coverage, as in a typical logistic regression. Note that in theory you could enter these as two column success and failure data frame, which is common in logistic regressions.

#3 use overdispersion: Chisq without overdispersion finds more true positives, but many more false positives. good compromise is overdispersion with Chisq. reduced true pos, but really reduces false pos rate.

# get all differentially methylated bases
myDiff=getMethylDiff(myDiff,difference=10,qvalue=0.05)

# we can visualize the changes in methylation frequencies quickly.
hist(getData(myDiff)$meth.diff)

# get hyper methylated bases
hyper=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hyper")
#
# get hypo methylated bases
hypo=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hypo")

```


### Plots of differentially methylated groups


```
#heatmaps first

# get percent methylation matrix
pm <- percMethylation(meth_sub)

# make a dataframe with snp id's, methylation, etc.
sig.in <- as.numeric(row.names(myDiff))
pm.sig <- pm[sig.in,]

# add snp, chr, start, stop

din <- getData(myDiff)[,1:3]
df.out <- cbind(paste(getData(myDiff)$chr, getData(myDiff)$start, sep=":"), din, pm.sig)
colnames(df.out) <- c("snp", colnames(din), colnames(df.out[5:ncol(df.out)]))
df.out <- (cbind(df.out,getData(myDiff)[,5:7]))

####
# heatmap
####

my_heatmap <- pheatmap(pm.sig,
        show_rownames = FALSE)

ctrmean <- rowMeans(pm.sig[,1:4])

h.norm <- (pm.sig-ctrmean)

my_heatmap <- pheatmap(h.norm,
        show_rownames = FALSE)

##### if you want to change colors. only because I don't love the default colors.

paletteLength <- 50
myColor <- colorRampPalette(c("cyan1", "black", "yellow1"))(paletteLength)
myBreaks <- c(seq(min(h.norm), 0, length.out=ceiling(paletteLength/2) + 1), 
              seq(max(h.norm)/paletteLength, max(h.norm), length.out=floor(paletteLength/2)))
              
my_heatmap <- pheatmap(h.norm,
        color=myColor, 
        breaks=myBreaks,
        show_rownames = FALSE)

#####
#let's look at methylation of specific gene or snp
####

df.out
df.plot <- df.out[,c(1,5:12)] %>% pivot_longer(-snp, values_to = "methylation")
df.plot$group <- substr(df.plot$name,1,2)
head(df.plot)

# looking at snp LS049205.1:248
# if you choose a different snp, you can create different plots.

df.plot %>% filter(snp=="LS049205.1:248") %>% 
            ggplot(., aes(x=group, y=methylation, color=group, fill=group)) +
              stat_summary(fun.data = "mean_se", size = 2) +
              geom_jitter(width = 0.1, size=3, pch=21, color="black")

## write bed file for intersection with genome annotation

write.table(file = "~/Documents/UVM/Ecological_genomics_teaching/diffmeth.bed",
          data.frame(chr= df.out$chr, start = df.out$start, end = df.out$end),
          row.names=FALSE, col.names=FALSE, quote=FALSE, sep="\t")
```

### link to genes

```
in bash, on server.

/data/popgen/bedtools2/bin/bedtools closest -a diffmeth.bed \
      -b /data/project_data/epigenetics/GCA_900241095.1_Aton1.0_genomic.fa_annotation_table.bed \
      -D b | \
      awk '!($10=="-")' > hits.bed 

# the annotation file here has the format: ScaffoldName  FromPosition  ToPosition  Sense TranscriptName  TranscriptPath  GeneAccession GeneName  GeneAltNames  GenePfam  GeneGo  CellularComponent MolecularFunction BiologicalProcess

# note that the hits.bed file will paste the diffmeth.bed file before the annotation table. So the first 3 columns are fom diffmeth.bed, then next 8 from the annotation table.

# count up number of hits

cat hits.bed | wc -l

# count number of unique named genes
cat hits.bed | cut -f 8 | sort | uniq -c



```
   
  

#### parameters of coverage files:
Column 1: Chromosome
Column 2: Start position
Column 3: End position
Column 4: Methylation percentage
Column 5: Number of methylated C's
Column 6: Number of unmethylated C's


#### methylki analysis





#### Notes:

You can type R and do R from a terminal

Also, you can do ` R script *****.R` to run an R script !

------    
<div id='id-section59'/>   


### Entry 59: 2020-04-02, Thursday.   



------    
<div id='id-section60'/>   


### Entry 60: 2020-04-03, Friday.   



------    
<div id='id-section61'/>   


### Entry 61: 2020-04-06, Monday.   



------    
<div id='id-section62'/>   


### Entry 62: 2020-04-07, Tuesday.   



------    
<div id='id-section63'/>   


### Entry 63: 2020-04-08, Wednesday.   



------    
<div id='id-section64'/>   


### Entry 64: 2020-04-09, Thursday.   



------    
<div id='id-section65'/>   


### Entry 65: 2020-04-10, Friday.   



------    
<div id='id-section66'/>   


### Entry 66: 2020-04-13, Monday.   



------    
<div id='id-section67'/>   


### Entry 67: 2020-04-14, Tuesday.   



------    
<div id='id-section68'/>   


### Entry 68: 2020-04-15, Wednesday.   



------    
<div id='id-section69'/>   


### Entry 69: 2020-04-16, Thursday.   



------    
<div id='id-section70'/>   


### Entry 70: 2020-04-17, Friday.   



------    
<div id='id-section71'/>   


### Entry 71: 2020-04-20, Monday.   



------    
<div id='id-section72'/>   


### Entry 72: 2020-04-21, Tuesday.   



------    
<div id='id-section73'/>   


### Entry 73: 2020-04-22, Wednesday.   



------    
<div id='id-section74'/>   


### Entry 74: 2020-04-23, Thursday.   



------    
<div id='id-section75'/>   


### Entry 75: 2020-04-24, Friday.   



------    
<div id='id-section76'/>   


### Entry 76: 2020-04-27, Monday.   



------    
<div id='id-section77'/>   


### Entry 77: 2020-04-28, Tuesday.   



------    
<div id='id-section78'/>   


### Entry 78: 2020-04-29, Wednesday.   



------    
<div id='id-section79'/>   


### Entry 79: 2020-04-30, Thursday.   



------    
<div id='id-section80'/>   


### Entry 80: 2020-05-01, Friday.   



------    
<div id='id-section81'/>   


### Entry 81: 2020-05-04, Monday.   



------    
<div id='id-section82'/>   


### Entry 82: 2020-05-05, Tuesday.   



------    
<div id='id-section83'/>   


### Entry 83: 2020-05-06, Wednesday.   



------    
<div id='id-section84'/>   


### Entry 84: 2020-05-07, Thursday.   



------    
<div id='id-section85'/>   


### Entry 85: 2020-05-08, Friday.   



